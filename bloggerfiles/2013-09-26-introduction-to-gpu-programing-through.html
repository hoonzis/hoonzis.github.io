---
layout: post
title: Introduction to GPU programing through AMP C++
date: '2013-09-26T15:05:00.001-07:00'
author: Jan Fajfr
tags:
- C++
- GPU
- Computer Science
modified_time: '2014-06-26T10:29:29.947-07:00'
thumbnail: http://3.bp.blogspot.com/-XNWrfsNIS7I/UkSvbyVrjYI/AAAAAAAAAsQ/UyOQkpJTkl8/s72-c/gpu_world.png
blogger_id: tag:blogger.com,1999:blog-1710034134179566048.post-1965487944885903339
blogger_orig_url: http://hoonzis.blogspot.com/2013/09/introduction-to-gpu-programing-through.html
---

<style type="text/css">table.sample {  border-width:1px;  border-style:solid;  border-color:black; } table.sample th {  border-width:1px;  border-style:solid;  border-color:black; } table.sample td {  border-width:1px;  border-style:solid;  border-color:black;  padding:0px; } </style><p>Few months ago I tried to learn a bit about GPU programming and I took notes a started to write this post. I am publishing this now even though it's not complete, however being too busy, I am not sure whether I will have the time to get back to this later.</p><p>Since couple years CUDA (*2007) and OpenCL (*2008) have established themselves as standard frameworks for parallel programming on the GPU. In 2011 new framework came to the landscape of GPGPU programming, which does not compete on the same level, but rather represents new abstraction between the GPU and the developer.</p><p>AMP stands for Accelerated Massive Parallelism and it is an open C++ standard. The implementation of AMP provided by Microsoft consists of three main features:</p><ul> <li>C++ language features and compiler</li> <li>Runtime</li> <li>Programming framework containing classes and functions to facilitate parallel programming</li></ul><p>As well as OpenCL and unlike CUDA, AMP is a standard not only for GPU programming but for any data-parallel hardware. Typical example of such a hardware is the vector unit of standard CPU, capable of executing SIMD (Single Instruction Multiple Data) instructions. Or in the near future a cloud based cluster. AMP is an abstraction layer which can be used to target different devices from different vendors.</p><p>The following diagram depicts how AMP C++ fits to the world of GPGPU programming.</p><a href="http://3.bp.blogspot.com/-XNWrfsNIS7I/UkSvbyVrjYI/AAAAAAAAAsQ/UyOQkpJTkl8/s1600/gpu_world.png" imageanchor="1" ><img border="0" src="http://3.bp.blogspot.com/-XNWrfsNIS7I/UkSvbyVrjYI/AAAAAAAAAsQ/UyOQkpJTkl8/s400/gpu_world.png" /></a><p>Microsoft has implemented AMP on top of DirectCompute technology and it is the only production ready implementation. Currently there are also two Proof Of Concepts AMP implementations <a href="http://isocpp.org/blog/2012/12/shevlin-park">Shelvin park project</a> done by Intel and <a href="https://github.com/corngood/llvm">open source modification of the LLVM compiler</a> (<a href="http://www.marshut.com/ipymi/c-amp-opencl-nvptx-prototype.html">more information here</a>). Needless to say that the adoption of such standard will depend a lot on whether these implementations will be taken further and whether we will have real implementation based on CUDA.</p><h2>First look at the code</h2><p>Without further introduction let's take a look at the classical "Hello World" of GPGPU programming: Addition of elements of two vectors (here without the boiler plate code).</p><pre class="prettyprint"><br />parallel_for_each(e,[=](index&lt;2&gt;idx) restrict(amp){<br /> c[idx] = a[idx] + b[idx];<br />});<br /></pre><p>If you have ever tried out OpenCL or CUDA, you should find this piece of code less verbose and quite easy to read. That is the result of the main ideas behind AMP C++:</p><ul> <li><b>Full C++</b> - unlike CUDA or OpenCL, AMP is not a C like dialect.</li> <li><b>Minimalist API</b> - AMP hides as much as it can from the developer.</li> <li><b>Future proofed</b> - the API is designed for heterogeneous programming. In the future GPU should be only one of it's usages. It aims to be a standard for programming on distributed systems.</li></ul><p>Before going futher with more examples we have to provide basic description of GPU architecture as it is crutial to understanding certain parts of AMP</p><h2>GPU architecture</h2><p>Here I will give a high level overview of the GPU architecture. If you are already familiar with the GPU architecture, feel free to skip this section.</p><p>Currently there are two main producers of GPU chips: NVidia and ATI (AMD). A little behind comes Intel. Each of them constructs graphic cards with different architecture. Moreover the architectures change significantly with the realease of any new version. Nevertheless certain concepts are shared by all of them. I will describe here shortly the NVidias Fermi and ATIs Cypress architecture. The succesor of Fermi is called <a href="http://www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf">Kepler</a>.</p><h3>Processor architecture</h3><p>The GPU is composed of hundreds of pipelined cores. The cores are grouped to computation units. NVidia calls these units <b>Symmetric Multiprocessors.</b> Each computation unit is assigned a unit of work.</p><a href="http://4.bp.blogspot.com/-aZCFecuHVPs/UkSvjHWnpJI/AAAAAAAAAsY/SbDYlzDG3Po/s1600/fermi.png" imageanchor="1" ><img border="0" src="http://4.bp.blogspot.com/-aZCFecuHVPs/UkSvjHWnpJI/AAAAAAAAAsY/SbDYlzDG3Po/s400/fermi.png" /></a><p>ATI uses slightly different abstraction. Similary to NVidia the "cores" are grouped to Symmetric Multiprocessors, however each core is called <b>Thread Processor</b> and is a capable of executing VLIV (Very Long Instruction Word) instructions. It has therefore 4 arithmetic logical units and one Special Function Unit. The compiler has to find independent operations and construct the VLIW instruction. If the compiler is not able to find these independent operations than one or more of the ALUs will not be performing any operation.</p><a href="http://2.bp.blogspot.com/-UtmWtZfmX6c/UkSvpeZehMI/AAAAAAAAAsg/ui4FBffCckA/s1600/cypress.png" imageanchor="1" ><img border="0" src="http://2.bp.blogspot.com/-UtmWtZfmX6c/UkSvpeZehMI/AAAAAAAAAsg/ui4FBffCckA/s400/cypress.png" /></a><h3>Memory access</h3><p>The GPU has a global memory which can be accessed by all cores. This access is quite costly (hundreds of cycles). Each simetric multiprocessor in turn has a small scratch-pad memory which is called usually <b>local memory</b>. This memory can be accessed only by the threads running on the given SM. The access to local memory is much cheaper (around 10 cycles). This memory can either take a role of a cache or can be managed by the developer directly. In addition each core has its own general purpose registers, used to perform the computations.</p><p>GPUs usually run computations on data which is accessed only once, unlike CPUs where the same memory pages are often used repetively. For this reason it is better for the developer to control the local memory. That's why historically the local memory did not act as cache on GPUs. However this will probably change in the future, and some form of caching will be provided by the GPU directly. <h3>Scheduling</h3><p>Scheduling is essential because it allows the effective usage of the processor. Since the memory access is expensive, other operations might be executed while the processor is waiting for the data to come. NVidia processors have "costless" context switching, so while one thread is blocked other can take over. Therefore the number of threads scheduled might affect the computation performance, while if not enough threads are available for scheduling some might be waiting for the memory page loads.</p> <h2>The programming models comparison:</h2><p>These architectural concepts are used by NVIDIA and ATI. Each of the 3 GPU programming models (CUDA, OpenCL, AMP) can has its own dialects and namings. The following table shows the terms used by all three technologies. We will discuss the AMP terms used in this table later in the article.</p><table class="sample"><thead><tr><td>Term</td><td>CUDA</td><td>AMP C++</td><td>OpenCL</td></tr></thead><tbody style="boder:1px solid"><tr><td>Basic unit of work</td><td>thread</td><td>thread</td><td>work-item</td></tr><tr><td>The code executed on one item</td><td>kernel</td><td>kernel</td><td>kernel</td></tr><tr><td>The unit processing one group of working units</td><td>Streaming multiprocessor</td><td>-</td><td>Compute unit</td></td><tr><td>Cache-style memory accessible by grouped threads</td><td>shared memory</td><td>tile static memory</td><td>local memory</td><tr><td>Group of working units sharing local memory</td><td>warp</td><td>tile</td><td>work-group</td></tr></tbody></table> <h3>The tools offered by each framework</h3><p>Now when the architecture of GPU has been described we can define the tools which each framework needs to provide. In the next part of the article I will try to describe how AMP adresses these issues.</p><ul> <li>Tell the compiler which part of the code will be offloaded to the GPU</li> <li>Provide constructs to work with multidimensional vectors</li> <li>Provide a way to transfer the data between the GPU and the CPU</li> <li>Give the developer tools to write efficient programs. That is to address GPU specific problems such as memory access</li></ul> <h3>AMP detailed description and more examples</h3><p>Here is again the first example with the little of boiler plate code that we have to write to make it work:</p><pre class="prettyprint"><br />void vectorsAddition_Parallel(vector&lt;float> vA, vector&lt;float> vB, vector&lt;float>vC, int M, int N){<br /> extent&lt;2&gt; e(M,N);<br /> array_view&lt;float,2&gt; a(e, vA), b(e,vB);<br /> array_view&lt;float,2&gt; c(e, vC);<br /><br /><br /> //capture of the data using array_view -&gt; results in the copy of the data into the accelerators memmory.<br /> parallel_for_each(e,[=](index&lt;2&gt;idx) restrict(amp){<br />  c[idx] = a[idx] + b[idx];<br /> });<br />}<br /></pre><p>The code sent and executed on the GPU is called "kernel" (here one line of code). The kernel is passed to the GPU in the form of lambda expression through the <b>parallel_for_each</b> method. This static method is the entry point to AMP. This method takes two parameters <b>parallel_for_each(extent, delegate)</b>. The extend parameter describes the dimensionality of the data. The delegate encapsulates the logic which will be executed. The logic is usually defined as an anonymous function which takes the <b>index&lt;N&gt;</b> as parameter. The computation is expressed using this index on previously defined arrays. In the aboved sample the <b>c[idx] = a[idx] + b[idx]</b> simply means, that for each index (and index goes from 0,0 to N,N since it is two dimensional index) the elements at these positions of the arrays will be added and stored in the array c. Of course that this code is not executed sequentially, but instead defined as a set of vector operations which are scheduled on the GPU</p> <p>The extent as well as the index parameter are templated. The index identifies one unique position in N dimensional array. The extent describes the dimensions of the computation domain.<p> <p>The <b>array_view</b> class takes care of the copy of the data to and from the GPU. When the computation is finished, the synchronized method is called on the <i>vC array_view</i>. This call will synchronize the C vector with the array_view. To give the complete information, note also that there is an <b>array</b> class which behaves similary, having few inconvenience and advantages. <a href="http://blogs.msdn.com/b/nativeconcurrency/archive/2012/07/17/choosing-between-array-and-array-view-in-c-amp.aspx">This post</a> gives a good comparison of these two classes.</p> <p>The following example ilustrates some of the dificulties which we can have when writing parallel code. The code simply sums all the elements in an array in parallel way. The parallelization of addition of items requires a bit of engineering, even though the sequential solution is evident. Here I am using a technique which is not really efficient but demonstrates the principles of parallelization. Several techniques to parallelize the task are described in <a href="http://blogs.msdn.com/b/nativeconcurrency/archive/2012/03/08/parallel-reduction-using-c-amp.aspx">this article</a>.</p><pre class="prettyprint"><br />float sumArray_NaiveAMP(std::vector&lt;float&gt; items){<br /> auto size = items.size();<br /> array_view&lt;float, 1&gt; aV (size, items);<br /> <br /> for(int i=1;i&lt;size;i=2*i){<br />  parallel_for_each(extent&lt;1&gt;(size/2), [=] (index&lt;1&gt; idx) restrict(amp)<br />  {<br />   aV[2*idx*i] = aV[2*idx*i] + aV[2*idx*i+i];<br />  });<br /> }<br /><br /> return aV[0];<br />}<br /></pre><p>The algorihtm adds each two neighbouring items and stores the result in the first item. This has to be repeated until the addition of the whole array is stored in the first position in the array. As described in the mentioned article this approach is not memory efficient and optimizations exists. Note also that the synchronize method is not called on the array_view at the end of the computation. That is because, we don't want the modified data to be copied back from the GPU to the main memory, we are only interested in the sum of the elements.</p> <p> Another example here is the computation of the standard deviation of the values in an array. First step is the computation of the avarage of the array. To obtain the avarege we have to first add the elements in the array (using the previous example). Having the average, we can obtain the distance of each element from the average. Once we have the distance of each element, we have to make another addition before taking the final square root and obtaining the standard deviation.</p><pre class="prettyprint"><br />float standatd_deviation(vector&lt;float&gt; vA) {<br /> <br /> float size = vA.size();<br /> <br /> extent&lt;1&gt; e((int)size);<br /> vector&lt;float&gt; vDistance(size);<br /> <br /> array_view&lt;float, 1&gt; a(e, vA);<br /> array_view&lt;float, 1&gt; distance(e,vDistance);<br /> <br /> float dispertion = 0;<br /> float total = 0; <br /> total = sumArray_NaiveAMP(vA);<br /> float avg = total/size;<br /><br /> parallel_for_each(<br />  e, <br />  [=](index&lt;1&gt; idx) restrict(amp) {<br />    distance[idx] = (a[idx] - avg)*(a[idx] - avg);<br /><br /> });<br /><br /> distance.synchronize();<br /> dispertion = sumArray_NaiveAMP(vDistance);<br /> return sqrt(dispertion/size);<br />}<br /></pre><p>This algorithm has 3 parallelized parts: the two sums at the beginning and at the end, and than the calculation of the distance of each element.</p><p>Looking at both of the preceding examples, you might be wondering why the code is so complex and you might think that the sum of the elements in the array could be just written as:</p><pre class="prettyprint"><br />float sum = 0;<br />parallel_for_each(<br /> e, <br /> [=](index&lt;1&gt; idx) restrict(amp) {<br />   sum+=a[idx];<br /><br />});<br /></pre><p>However if you think a bit more, you should understand that the code in the <b>parallel_for_each</b> runs essential in the same time. All the parallel threads would like to increment the <i>sum</i> variable at the same time. In addition to that, this code would not even compile, while the modifications of variables captured "by value" are not allowed and in this example the <i>sum</i> variable is captured by value. If you are not familiar with the different capture types refer to <a href="http://msdn.microsoft.com/en-us/library/dd293603.aspx">this page.</a> </p> <p>Here is one more example which ilustrates how index and extent work, it is the second hello world of parallel computing: matrix multiplication. This example come from <a href="http://msdn.microsoft.com/en-us/library/hh873134.aspx">this MSDN page</a>.</p><pre class="prettyprint"><br />void matrixMultiplicationWithAMP(vector&lt;float&gt; &vC, vector&lt;float&gt; vA, vector&lt;float&gt; vB, int M, int N) {<br /> <br /> extent&lt;2&gt; e(M,N);<br /><br /> array_view&lt;float, 2&gt; a(e, vA);<br /> array_view&lt;float, 2&gt; b(e,vB);<br /> array_view&lt;float, 2&gt; product(e, vC);<br /> <br /><br /> parallel_for_each(<br />  product.extent, <br />  [=](index&lt;2&gt; idx) restrict(amp) {<br />   int row = idx[0];<br />   int col = idx[1];<br />   for (int inner = 0; inner &lt; M; inner++) {<br />    product[idx] += a(row,inner) * b(inner,col);<br />   }<br /> }<br /> );<br /><br /> product.synchronize();<br />}<br /></pre><p>Note that the resulting vector <i>vC</i> i passed to the method as reference, since it's content is modified by the <i>synchronize</i> call. Also note, that this example assumes that the vectors passed to the function contain two dimensional array of size (N,N). Since AMP supports multidimensional indexes, AMP runs over all the columns and all the rows automatically, just by iterating over the two-dimensional index. The inner loop just sums the multiplications of the elements in the current row of the left matrix and the current column of the right matrix.</p><h3>Moving the data between GPU and CPU</h3><p>As mentioned before, the <i>array_view</i> and <i>array</i> classes are used to transfer the data between the CPU and GPU. The <i>array</i> class directly copies the data to the GPUs global memory. However the data from the <i>array</i> has to be then sent manually back to the CPUs memmory. On the other hand the <i>array_view</i> class works as a wrapper. The vector passed to the <i>array_view</i> will in the background copy the data from and to the vector which is passed in as parameter.</p> <h3>Memory access and manipulation on AMP</h3><p>As described above, the developer has to address the GPU and adapt the algorithm to the architecture. This basically means minimize the access to global memmory and optimize the threads to use the local memmory. This process is called <b>tiling</b> in the AMP's parlance and the local memmory is called <b>tile-static</b> memory.</p><p>If the developer does not define any tiling, the code will be tiled by default. In order to use the local memory efficiently, algorithm has to be tiled manualy. Parallel_for_each method has a second overload which accepts <i>tile_extent</i> as a parameter and the code receives <i>tiled_index</i> in the lambda. Similary as the <i>extend</i> the <i>tile_extend</i> specifies the dimensionality of the computation domain, but also separates the whole computation domain into several tiles. Each tile is than treated by one symetrical multiprocessor, therefor all the threads in the tile can share the local memory and benefit from the fast memory access. If you want to read a bit more about tiling visit <a href="http://msdn.microsoft.com/en-us/magazine/hh882447.aspx">this page.</a></p><h2>About the future of AMP</h2>As said at the beginning AMP is a standard and as any standard it is dependent of it's implementations. Currently there are two existing implementations of the AMP standard. Microsoft implemented AMP on top of Direct Compute technology. Direct Compute is a part of Microsoft's DirectX suite, which was originally suited only to multimedia programming. Microsoft added DirectComputed to the DirectX suite in order to enable GPGPU programming and with AMP C++ provides an easy way to manipulate the API. The second implementation is very recent and was developed by Intel under the code name Shelvin Park. This implementation builds on top of OpenCL. <h2>Summary</h2><p>Clearly the success of the standard depends on whether other implementations targeting CUDA and OpenCL will emerge. Microsoft cooperated with NVidia and AMD during the development of the API. The idea of having a clear C++ standard to define the parallel computation is great. Latest C++ is quite modern language and provides nice constructs, so actually the programming using AMP C++ is quite fun and not that much pain.</p> <h3>Links</h3><a href="http://geekswithblogs.net/JoshReuben/archive/2011/12/04/c-amp.aspx">Introduction and few samples</a><a href="http://blogs.msdn.com/b/nativeconcurrency">Parallel Programing in Native Code - blog of the AMP team</a><p><a href="http://www.anandtech.com/show/6846/nvidia-updates-gpu-roadmap-announces-volta-family-for-beyond-2014">CUDA architecture evolution</a></p><p><a href="http://www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf">GeForce GTX 680 Kepler Architecture</a></p><p><a href="http://www.streamcomputing.eu/blog/2011-06-22/opencl-vs-cuda-misconceptions/">Comparison between CUDA and OpenCL (though little outdated 2011-06-22)</a></p><p><a href="http://msdn.microsoft.com/en-us/library/hh553049.aspx">http://msdn.microsoft.com/en-<wbr>us/library/hh553049.aspx</a></p><p><a href="http://msdn.microsoft.com/en-us/magazine/hh882447.aspx">Introduction to Tiling</a></p><p><a href="https://github.com/corngood">Implementation of LLVM & Clang to support AMP C++ on NVidia</a></p> 